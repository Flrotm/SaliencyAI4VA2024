{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9118840,"sourceType":"datasetVersion","datasetId":5504490},{"sourceId":9182754,"sourceType":"datasetVersion","datasetId":5550310},{"sourceId":196686333,"sourceType":"kernelVersion"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom scipy.misc import face\nfrom scipy.ndimage import zoom\nfrom scipy.special import logsumexp\nimport torch\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom sklearn.metrics import roc_auc_score\nfrom scipy.stats import gaussian_kde\nDEVICE = \"cuda\"\n\ndef calculate_auc(pred, gt):\n    gt_binary = (gt > 0.5).astype(int).flatten()\n    pred = pred.flatten()\n    auc = roc_auc_score(gt_binary, pred)\n    return auc\n\ndef calculate_nss(pred, gt):\n    pred_mean = np.mean(pred)\n    pred_std = np.std(pred)\n    if pred_std == 0:\n        return 0\n    pred_normalized = (pred - pred_mean) / pred_std\n    nss = np.mean(pred_normalized * gt)\n    return nss\n\ndef calculate_cc(pred, gt):\n    pred = pred.flatten()\n    gt = gt.flatten()\n    cc, _ = pearsonr(pred, gt)\n    return cc\n\ndef calculate_kld(pred, gt, epsilon=1e-10):\n    pred = pred + epsilon\n    gt = gt + epsilon\n    \n    pred = pred / np.sum(pred)\n    gt = gt / np.sum(gt)\n    \n    kld = entropy(gt, pred)\n    return kld","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-15T11:59:05.313596Z","iopub.execute_input":"2024-09-15T11:59:05.314245Z","iopub.status.idle":"2024-09-15T11:59:07.762165Z","shell.execute_reply.started":"2024-09-15T11:59:05.314212Z","shell.execute_reply":"2024-09-15T11:59:07.761272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport easyocr\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\n\nimages_folder = \"/kaggle/input/saliency-data/images/val\"\n\nimage_ids = [\n    \"Vaillant_0480_1954_07_25-01\",\n    \"Vaillant_0608_1957_01_06-06\",\n    \"Vaillant_0525_1955_06_05-16\"\n]\n\n# fr = french\nreader = easyocr.Reader(['fr'], gpu=True)\n\n\ndef detect_text_boxes_with_easyocr(image):\n    image_np = np.array(image)\n\n    results = reader.readtext(image_np)\n\n    boxes = []\n    for (bbox, text, prob) in results:\n        boxes.append(bbox)\n\n    return boxes\n\n\ndef show_image_and_text_boxes(image_id):\n\n    image_path = os.path.join(images_folder, f\"{image_id}.png\")\n    image = Image.open(image_path).convert('RGB')\n    boxes = detect_text_boxes_with_easyocr(image)\n    image_np = np.array(image)\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image_np)\n    plt.axis(\"off\")\n\n    \n    for box in boxes:\n        box = np.int0(box) \n        cv2.polylines(image_np, [np.array(box)], isClosed=True, color=(0, 255, 0), thickness=2)\n\n\n    plt.imshow(image_np)\n    plt.title(\"Text Detection with EasyOCR\")\n    plt.axis(\"off\")\n    plt.show()\n\n\nfor image_id in image_ids:\n    show_image_and_text_boxes(image_id)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:22:22.490325Z","iopub.execute_input":"2024-09-14T17:22:22.490679Z","iopub.status.idle":"2024-09-14T17:22:39.957240Z","shell.execute_reply.started":"2024-09-14T17:22:22.490649Z","shell.execute_reply":"2024-09-14T17:22:39.956231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"comic_images_folder = '/kaggle/input/saliency-data/images/val/'\nannotations_folder = '/kaggle/input/saliency-data/maps/val/'","metadata":{"execution":{"iopub.status.busy":"2024-09-14T16:42:27.136156Z","iopub.execute_input":"2024-09-14T16:42:27.136763Z","iopub.status.idle":"2024-09-14T16:42:27.141825Z","shell.execute_reply.started":"2024-09-14T16:42:27.136733Z","shell.execute_reply":"2024-09-14T16:42:27.140928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comic_images = sorted([f for f in os.listdir(comic_images_folder) if f.endswith('.png') or f.endswith('.jpg')])\nannotations = sorted([f for f in os.listdir(annotations_folder) if f.endswith('.png') or f.endswith('.jpg')])\n\n# Ensure the lists are sorted and matched correctly\nassert len(comic_images) == len(annotations), \"The number of images and annotations should be the same.\"","metadata":{"execution":{"iopub.status.busy":"2024-09-14T16:42:31.529842Z","iopub.execute_input":"2024-09-14T16:42:31.530233Z","iopub.status.idle":"2024-09-14T16:42:31.540006Z","shell.execute_reply.started":"2024-09-14T16:42:31.530201Z","shell.execute_reply":"2024-09-14T16:42:31.539105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_images_side_by_side(comic_img_path, annotation_img_path):\n    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n    \n    comic_img = mpimg.imread(comic_img_path)\n    annotation_img = mpimg.imread(annotation_img_path)\n    \n    axs[0].imshow(comic_img)\n    axs[0].set_title('Comic Image')\n    axs[0].axis('off')\n    \n    axs[1].imshow(annotation_img, cmap='gray')\n    axs[1].set_title('Saliency Annotation')\n    axs[1].axis('off')\n    \n    plt.show()\n    \ni=0\nfor comic_img, annotation_img in zip(comic_images, annotations):\n    display_images_side_by_side(os.path.join(comic_images_folder, comic_img), \n                                os.path.join(annotations_folder, annotation_img))\n    i+=1\n    if i==1:\n        break # Remove or modify this line to display more images","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:23:06.596316Z","iopub.execute_input":"2024-09-14T17:23:06.597008Z","iopub.status.idle":"2024-09-14T17:23:07.603791Z","shell.execute_reply.started":"2024-09-14T17:23:06.596973Z","shell.execute_reply":"2024-09-14T17:23:07.602834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport os\n\n# Paths to your data directories\ntrain_images_folder = '/kaggle/input/saliency-data/images/train/'\ntrain_annotations_folder = '/kaggle/input/saliency-data/maps/train/'\nval_images_folder = '/kaggle/input/saliency-data/images/val/'\nval_annotations_folder = '/kaggle/input/saliency-data/maps/val/'\n\n\n\ndef print_image_sizes(img_folder, map_folder):\n    img_ids = os.listdir(img_folder)\n    \n    for img_id in img_ids:\n        img_path = os.path.join(img_folder, img_id)\n        map_path = os.path.join(map_folder, img_id)\n        \n       \n        img = Image.open(img_path)\n        sal_map = Image.open(map_path)\n        \n       \n        print(f\"Image: {img_path}, Size: {img.size}\")\n        print(f\"Saliency Map: {map_path}, Size: {sal_map.size}\")\n        print(\"-\" * 50)\n\n\nprint(\"Training Set Sizes:\")\nprint_image_sizes(train_images_folder, train_annotations_folder)\n\n\nprint(\"Validation Set Sizes:\")\nprint_image_sizes(val_images_folder, val_annotations_folder)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-14T17:23:14.201499Z","iopub.execute_input":"2024-09-14T17:23:14.202329Z","iopub.status.idle":"2024-09-14T17:23:14.287602Z","shell.execute_reply.started":"2024-09-14T17:23:14.202295Z","shell.execute_reply":"2024-09-14T17:23:14.286697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport easyocr\nimport cv2\nimport torch\nimport matplotlib.pyplot as plt\n\nclass SaliconDatasetWithTextChannel(Dataset):\n    def __init__(self, img_dir, gt_dir, img_ids, exten='.png', augment=False):\n        self.img_dir = img_dir\n        self.gt_dir = gt_dir\n        self.img_ids = img_ids\n        self.exten = exten\n        self.augment = augment\n        \n        self.img_transform = transforms.Compose([\n            transforms.Resize((512, 512)),  # Resize to 512x512\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n        ])\n        \n        self.gt_transform = transforms.Compose([\n            transforms.Resize((512, 512)),  # Resize to 512x512\n            transforms.ToTensor()\n        ])\n        \n        # Augmentation transforms\n        self.augmentation_transform = transforms.Compose([\n            transforms.RandomResizedCrop(512, scale=(0.8, 1.0)),  # Random crop and resize\n        ])\n\n        # Initialize EasyOCR reader for French\n        self.reader = easyocr.Reader(['fr'], gpu=True)\n    def detect_text_boxes_with_easyocr(self, image, area_threshold=7000):\n        \"\"\"\n        Detect text boxes using EasyOCR and filter out large text boxes based on an area threshold.\n        Args:\n        - image: PIL image to perform text detection on.\n        - area_threshold: the area size above which the text boxes are ignored (in pixels).\n\n        Returns:\n        - text_mask: a binary mask with text regions marked (as a PIL image).\n        \"\"\"\n\n        image_np = np.array(image)\n        results = self.reader.readtext(image_np)\n\n        text_mask = np.zeros_like(image_np[:, :, 0])\n\n        \n        for (bbox, text, prob) in results:\n            # bbox is a list of four points, each point is a list [x, y]\n            # Extract the x and y coordinates for each corner of the bounding box\n            x0, y0 = bbox[0]  # top-left corner\n            x1, y1 = bbox[1]  # top-right corner\n            x2, y2 = bbox[2]  # bottom-right corner\n            x3, y3 = bbox[3]  # bottom-left corner\n\n            # Calculate width and height of the bounding box\n            width = np.linalg.norm([x1 - x0, y1 - y0]) \n            height = np.linalg.norm([x2 - x1, y2 - y1])  \n            area = width * height\n\n            #print(f\"Text: '{text}', Area: {area:.2f}, Width: {width:.2f}, Height: {height:.2f}\")\n\n            \n            if area < area_threshold:\n                pts = np.array(bbox, dtype=np.int32)\n                cv2.fillPoly(text_mask, [pts], 255)\n\n        return Image.fromarray(text_mask)  # Convert back to PIL Image\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        img_path = os.path.join(self.img_dir, img_id + self.exten)\n        gt_path = os.path.join(self.gt_dir, img_id + self.exten)\n\n        img = Image.open(img_path).convert('RGB')\n        gt = Image.open(gt_path).convert('L')\n\n        text_mask = self.detect_text_boxes_with_easyocr(img)\n\n        if self.augment:\n            img = self.augmentation_transform(img)\n            gt = self.augmentation_transform(gt)\n        \n        img = self.img_transform(img)\n        gt = self.gt_transform(gt)\n        \n        \n        text_mask = text_mask.resize((512, 512)) \n        text_mask = transforms.ToTensor()(text_mask)\n\n        # Combine image and text mask into a multi-channel tensor (4 channels now)\n        img_with_text_channel = torch.cat([img, text_mask], dim=0)\n\n        return img_with_text_channel, gt\n\n    def __len__(self):\n        return len(self.img_ids)\n\n\ntrain_images_folder = '/kaggle/input/saliency-data/images/train/'\ntrain_annotations_folder = '/kaggle/input/saliency-data/maps/train/'\nval_images_folder = '/kaggle/input/saliency-data/images/val/'\nval_annotations_folder = '/kaggle/input/saliency-data/maps/val/'\n\n\ntrain_img_ids = os.listdir(train_images_folder)\ntrain_img_ids = [os.path.splitext(f)[0] for f in train_img_ids]\n\nval_img_ids = os.listdir(val_images_folder)\nval_img_ids = [os.path.splitext(f)[0] for f in val_img_ids]\n\n\ntrain_dataset = SaliconDatasetWithTextChannel(train_images_folder, train_annotations_folder, train_img_ids, augment=False)\nval_dataset = SaliconDatasetWithTextChannel(val_images_folder, val_annotations_folder, val_img_ids, augment=False)\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True)\nval_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n\n\nfor images, targets in val_dataloader:\n    text_mask_np = images[0, 3, :, :].cpu().numpy()  # The 4th channel (index 3) corresponds to the text mask\n\n    plt.imshow(text_mask_np, cmap='gray')\n    plt.title(\"Text Detection Mask\")\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T11:59:23.278235Z","iopub.execute_input":"2024-09-15T11:59:23.278783Z","iopub.status.idle":"2024-09-15T11:59:58.722529Z","shell.execute_reply.started":"2024-09-15T11:59:23.278749Z","shell.execute_reply":"2024-09-15T11:59:58.721565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\n\nclass ViTSaliencyModel(nn.Module):\n    def __init__(self, freeze_vit_layers=True):\n        super(ViTSaliencyModel, self).__init__()\n        \n\n        self.backbone = timm.create_model('vit_base_patch16_384', pretrained=True, img_size=512)\n        self.backbone.head = nn.Identity()  # Removing the final classification head\n\n        self.backbone.patch_embed.proj = nn.Conv2d(4, 768, kernel_size=16, stride=16)  # For 4 input channels\n\n        \n        if freeze_vit_layers:\n            for param in self.backbone.parameters():\n                param.requires_grad = False\n\n        # CNN Feature Extraction for Local Features (modified to handle 4-channel input)\n        self.cnn_extractor = nn.Sequential(\n            nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3), \n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n        )\n\n        # Upsampling layers to reconstruct the saliency map, ensuring output is 512x512\n        self.upsample = nn.Sequential(\n            nn.Conv2d(768 + 128, 512, kernel_size=3, padding=1),  # Concatenated ViT and CNN features\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 1, kernel_size=1),  # Output single-channel saliency map\n            nn.Upsample(size=(512, 512), mode='bilinear', align_corners=False)\n        )\n\n\n        self.upsample.apply(self.weights_init)\n\n    def forward(self, x):\n        \n        cnn_features = self.cnn_extractor(x)\n\n        x = self.backbone.patch_embed(x) \n        x = self.backbone.pos_drop(x)\n        x = self.backbone.blocks(x) \n        x = self.backbone.norm(x) \n\n        # Reshape ViT output for upsampling\n        batch_size, num_patches, embedding_dim = x.size()\n        height = width = int(num_patches ** 0.5)\n\n        x = x.permute(0, 2, 1).contiguous()  # Rearrange to [batch, embedding_dim, height, width]\n        x = x.view(batch_size, embedding_dim, height, width)\n\n        # Ensure the CNN feature map has the same spatial dimensions as the ViT output\n        cnn_features = torch.nn.functional.interpolate(cnn_features, size=(height, width), mode='bilinear', align_corners=False)\n\n        # Concatenate ViT and CNN features\n        combined_features = torch.cat([x, cnn_features], dim=1)\n\n        # Upsample to generate saliency map\n        saliency_map = self.upsample(combined_features)\n        return saliency_map\n\n    # Weight initialization for upsampling layers\n    def weights_init(self, m):\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T12:00:11.795496Z","iopub.execute_input":"2024-09-15T12:00:11.796055Z","iopub.status.idle":"2024-09-15T12:00:12.949005Z","shell.execute_reply.started":"2024-09-15T12:00:11.796026Z","shell.execute_reply":"2024-09-15T12:00:12.948132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn as nn\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = ViTSaliencyModel(freeze_vit_layers=True).to(device)\n\ncriterion = nn.L1Loss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n\nnum_epochs = 9\nfor epoch in range(num_epochs):\n    model.train() \n    running_loss = 0.0\n    \n    for images, targets in train_dataloader:\n        images, targets = images.to(device), targets.to(device)\n        optimizer.zero_grad() \n        outputs = model(images)\n        loss = criterion(outputs, targets)\n        loss.backward() \n        optimizer.step()\n        running_loss += loss.item()\n    \n    epoch_loss = running_loss / len(train_dataloader)\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n    \n    scheduler.step(epoch_loss)\n\nprint('Finished Training')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T12:49:01.251896Z","iopub.execute_input":"2024-09-15T12:49:01.252669Z","iopub.status.idle":"2024-09-15T12:56:14.084724Z","shell.execute_reply.started":"2024-09-15T12:49:01.252636Z","shell.execute_reply":"2024-09-15T12:56:14.083764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-09-15T12:56:19.130675Z","iopub.execute_input":"2024-09-15T12:56:19.131045Z","iopub.status.idle":"2024-09-15T12:56:19.964596Z","shell.execute_reply.started":"2024-09-15T12:56:19.131014Z","shell.execute_reply":"2024-09-15T12:56:19.963385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport cv2\nimport numpy as np\nfrom torchvision import transforms\nimport easyocr\nfrom PIL import Image\n\ndef create_directory(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n\nreader = easyocr.Reader(['fr'], gpu=True)\n\ndef detect_text_boxes_with_easyocr(image, area_threshold=6000):\n    \"\"\"\n    Detect text boxes using EasyOCR and filter out large text boxes based on an area threshold.\n    Returns:\n    - text_mask: a binary mask with text regions marked (as a numpy array).\n    \"\"\"\n    image_np = np.array(image)\n    results = reader.readtext(image_np)\n    text_mask = np.zeros_like(image_np[:, :, 0])\n\n    for (bbox, text, prob) in results:\n        x0, y0 = bbox[0]\n        x1, y1 = bbox[1]\n        x2, y2 = bbox[2]\n        x3, y3 = bbox[3]\n        width = np.linalg.norm([x1 - x0, y1 - y0])\n        height = np.linalg.norm([x2 - x1, y2 - y1])\n        area = width * height\n\n        if area < area_threshold:\n            pts = np.array(bbox, dtype=np.int32)\n            cv2.fillPoly(text_mask, [pts], 255)\n\n    return text_mask\n\ndef add_text_channel(images):\n    processed_images = []\n    for img in images:\n        if img.shape[0] == 3:\n            img_pil = transforms.ToPILImage()(img.cpu())\n            text_mask = detect_text_boxes_with_easyocr(img_pil)\n            text_mask_resized = cv2.resize(text_mask, (img_pil.size[0], img_pil.size[1]))\n            text_mask_resized = text_mask_resized / 255.0\n            text_mask_tensor = transforms.ToTensor()(text_mask_resized).to(img.device)\n            img_with_text_channel = torch.cat([img, text_mask_tensor], dim=0)\n        else:\n            img_with_text_channel = img\n        \n        processed_images.append(img_with_text_channel)\n    \n    return torch.stack(processed_images)\n\n# Function to apply Gaussian blur to the saliency map\ndef apply_gaussian_blur(saliency_map, kernel_size=(5, 5)):\n    return cv2.GaussianBlur(saliency_map, kernel_size, 0)\n\n# Function to process and save saliency maps using the validation DataLoader\ndef process_validation_data(val_dataloader, output_folder, filenames):\n    create_directory(output_folder)\n\n    for i, (images, _) in enumerate(val_dataloader):\n        images = images.to(device)\n        images_with_text_channel = add_text_channel(images)\n\n        with torch.no_grad():\n            saliency_maps = model(images_with_text_channel)\n\n        # Process each image in the batch\n        for j in range(images.size(0)):\n            saliency_map = saliency_maps[j].squeeze().cpu().numpy()\n            # Normalize the saliency map between 0 and 1\n            saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n            # Apply Gaussian blur to the saliency map\n            saliency_map = apply_gaussian_blur(saliency_map)\n            # Convert to uint8 for saving\n            saliency_map = (saliency_map * 255).astype(\"uint8\")\n            \n            filename = filenames[i * images.size(0) + j]\n            output_path = os.path.join(output_folder, filename)\n            success = cv2.imwrite(output_path, saliency_map)\n            if success:\n                print(f'Successfully saved {filename} with shape {saliency_map.shape} to {output_path}')\n            else:\n                print(f'Failed to save image for {filename}')\n\n\nMODE = \"PRED\"\n\nif MODE == \"PRED\":\n    output_folder = \"/kaggle/working/preds9\"\nelse:\n    output_folder = \"/kaggle/working/preds/val\"\n\nfilenames = [f\"{name}.png\" for name in val_img_ids]\n\n\nprocess_validation_data(val_dataloader, output_folder, filenames)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T12:56:24.013112Z","iopub.execute_input":"2024-09-15T12:56:24.013484Z","iopub.status.idle":"2024-09-15T12:56:50.339023Z","shell.execute_reply.started":"2024-09-15T12:56:24.013455Z","shell.execute_reply":"2024-09-15T12:56:50.338100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom skimage import io\nfrom sklearn.metrics import roc_auc_score\nfrom scipy.stats import pearsonr, entropy\nimport cv2  \nimport matplotlib.pyplot as plt \n\ndef load_images_from_folder(folder):\n    images = {}\n    for filename in os.listdir(folder):\n        if filename.endswith(\".png\") or filename.endswith(\".jpg\"):  # Adjust based on your image format\n            img = io.imread(os.path.join(folder, filename), as_gray=True)\n            if img is not None:\n                images[filename] = img\n    return images\n\ndef normalize_map(saliency_map):\n    norm_map = (saliency_map - np.min(saliency_map)) / (np.max(saliency_map) - np.min(saliency_map) + 1e-10)\n    return norm_map\n\ndef apply_post_processing(saliency_map, method=\"\", kernel_size=(5, 5)):\n    \"\"\"\n    Apply post-processing to the saliency map.\n    You can extend this method to include different types of post-processing.\n    \"\"\"\n    if method == \"gaussian\":\n        return cv2.GaussianBlur(saliency_map, kernel_size, 0)\n    elif method == \"normalize\":\n        return normalize_map(saliency_map)\n    elif method == \"threshold\":\n        _, thresh_map = cv2.threshold(saliency_map, 0.8, 1.0, cv2.THRESH_BINARY)\n        return thresh_map\n    else:\n        return saliency_map \n\ndef calculate_auc(pred, gt):\n    pred = normalize_map(pred).flatten()\n    gt_binary = (gt > 0.5).astype(int).flatten()  # Binarize ground truth\n    return roc_auc_score(gt_binary, pred)\n\ndef calculate_nss(pred, gt):\n    gt_fixation = (gt > 0.5).astype(np.float32)  # Binarize fixation map\n    pred = normalize_map(pred)\n    mean_pred = np.mean(pred)\n    std_pred = np.std(pred)\n    if std_pred == 0:\n        return 0\n    pred_norm = (pred - mean_pred) / (std_pred + 1e-10)\n    return np.mean(pred_norm * gt_fixation)\n\ndef calculate_cc(pred, gt):\n    pred = normalize_map(pred)\n    gt = normalize_map(gt)\n    return pearsonr(pred.flatten(), gt.flatten())[0]\n\ndef calculate_kld(pred, gt):\n    pred = normalize_map(pred).flatten()\n    gt = normalize_map(gt).flatten()\n    return entropy(gt + 1e-10, pred + 1e-10)\n\ndef check_nan_or_zero(img, img_type):\n    if np.isnan(img).any():\n        print(f\"{img_type} contains NaN values\")\n    if np.all(img == 0):\n        print(f\"{img_type} is all zeros\")\n\ndef visualize_prediction(pred, gt, image_idx, filename):\n    \"\"\"Visualize the prediction and ground truth.\"\"\"\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.title(f\"Prediction {image_idx}: {filename}\")\n    plt.imshow(pred, cmap='gray')\n    plt.subplot(1, 2, 2)\n    plt.title(f\"Ground Truth {image_idx}: {filename}\")\n    plt.imshow(gt, cmap='gray')\n    plt.show()\n\ndef evaluate_model(predictions_folder, gt_folder, post_process_method=None):\n    predictions = load_images_from_folder(predictions_folder)\n    ground_truths = load_images_from_folder(gt_folder)\n\n\n    common_filenames = list(set(predictions.keys()).intersection(set(ground_truths.keys())))\n    common_filenames.sort()\n\n    auc_scores = []\n    nss_scores = []\n    cc_scores = []\n    kld_scores = []\n\n    for i, filename in enumerate(common_filenames):\n        pred = predictions[filename]\n        gt = ground_truths[filename]\n\n        check_nan_or_zero(pred, \"Prediction\")\n        check_nan_or_zero(gt, \"Ground Truth\")\n\n\n        if pred.shape != gt.shape:\n            print(f\"Resizing prediction {i} from shape {pred.shape} to {gt.shape}\")\n            pred = cv2.resize(pred, (gt.shape[1], gt.shape[0]))\n\n\n        if post_process_method:\n            pred = apply_post_processing(pred, method=post_process_method)\n\n        visualize_prediction(pred, gt, i, filename)\n\n\n        auc = calculate_auc(pred, gt)\n        nss = calculate_nss(pred, gt)\n        cc = calculate_cc(pred, gt)\n        kld = calculate_kld(pred, gt)\n        \n        print(f\"Image {i} ({filename}): AUC={auc}, NSS={nss}, CC={cc}, KLD={kld}\")\n        \n        auc_scores.append(auc)\n        nss_scores.append(nss)\n        cc_scores.append(cc)\n        kld_scores.append(kld)\n\n    metrics = {\n        'AUC': np.mean(auc_scores),\n        'NSS': np.mean(nss_scores),\n        'CC': np.mean(cc_scores),\n        'KLD': np.mean(kld_scores)\n    }\n\n    return metrics\n\n\npredictions_folder = \"/kaggle/working/preds9\"  # Path where your model predictions are saved\ngt_folder = \"/kaggle/input/saliency-data/maps/val\"  # Path to your ground truth saliency maps\n\nmetrics = evaluate_model(predictions_folder, gt_folder, post_process_method=\"gaussian\")\nprint(f\"Model Evaluation Metrics with Post-Processing:\\nAUC: {metrics['AUC']}\\nNSS: {metrics['NSS']}\\nCC: {metrics['CC']}\\nKLD: {metrics['KLD']}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T12:59:09.488095Z","iopub.execute_input":"2024-09-15T12:59:09.488495Z","iopub.status.idle":"2024-09-15T12:59:13.877802Z","shell.execute_reply.started":"2024-09-15T12:59:09.488461Z","shell.execute_reply":"2024-09-15T12:59:13.874601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom scipy.ndimage import gaussian_filter\n\n\nimages_folder = \"/kaggle/input/saliency-data/images/val\"\ngt_folder = \"/kaggle/input/saliency-data/maps/val\"\npredictions_folder = \"/kaggle/working/preds9\"\n\n\nimage_ids = [\n    \"Vaillant_0480_1954_07_25-01\",\n    \"Vaillant_0608_1957_01_06-06\",\n    \"Vaillant_0525_1955_06_05-16\",\n    \"Vaillant_0471_1954_05_23-14\",\n    \"Vaillant_0479_1954_07_18-01\",\n    \"Vaillant_0553_1955_12_18-06\",\n    \"Vaillant_0485_1954_08_29-01\"\n]\n\n\ndef apply_gaussian_smoothing(image, sigma=2):\n    return gaussian_filter(image, sigma=sigma)\n\n\ndef show_comic_gt_pred_smooth(image_id, sigma=2):\n    # Load the images\n    image_path = os.path.join(images_folder, f\"{image_id}.png\")\n    gt_path = os.path.join(gt_folder, f\"{image_id}.png\")\n    pred_path = os.path.join(predictions_folder, f\"{image_id}.png\")\n    \n    image = Image.open(image_path).convert(\"RGB\")\n    gt = Image.open(gt_path).convert(\"L\")  \n    pred = Image.open(pred_path).convert(\"L\") \n    pred_np = np.array(pred)\n    \n\n    pred_smoothed = apply_gaussian_smoothing(pred_np, sigma=sigma)\n    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n    \n    axs[0].imshow(image)\n    axs[0].set_title(\"Original Comic\")\n    axs[0].axis(\"off\")\n    \n    axs[1].imshow(gt, cmap=\"gray\")\n    axs[1].set_title(\"Ground Truth\")\n    axs[1].axis(\"off\")\n    \n    axs[2].imshow(pred, cmap=\"gray\")\n    axs[2].set_title(\"Prediction\")\n    axs[2].axis(\"off\")\n    \n    axs[3].imshow(pred_smoothed, cmap=\"gray\")\n    axs[3].set_title(f\"Prediction (Smoothed, Ïƒ={sigma})\")\n    axs[3].axis(\"off\")\n    \n    plt.show()\n\n\nfor image_id in image_ids:\n    show_comic_gt_pred_smooth(image_id, sigma=2)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T12:47:07.050347Z","iopub.execute_input":"2024-09-15T12:47:07.050985Z","iopub.status.idle":"2024-09-15T12:47:07.636364Z","shell.execute_reply.started":"2024-09-15T12:47:07.050952Z","shell.execute_reply":"2024-09-15T12:47:07.635098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn as nn\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel1 = ViTSaliencyModel(freeze_vit_layers=True).to(device)\n\ncriterion = nn.L1Loss()\noptimizer = optim.Adam(model1.parameters(), lr=0.001)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n\nnum_epochs = 12\nfor epoch in range(num_epochs):\n    model1.train()  \n    running_loss = 0.0\n    \n    for images, targets in train_dataloader:\n        images, targets = images.to(device), targets.to(device)\n        optimizer.zero_grad()  \n        outputs = model1(images)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step() \n        running_loss += loss.item()\n    \n    epoch_loss = running_loss / len(train_dataloader)\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n    \n    scheduler.step(epoch_loss)\n\nprint('Finished Training')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport cv2\nimport numpy as np\nfrom torchvision import transforms\nimport easyocr\nfrom PIL import Image\n\ndef create_directory(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\nreader = easyocr.Reader(['fr'], gpu=True)\n\ndef detect_text_boxes_with_easyocr(image, area_threshold=6000):\n    \"\"\"\n    Detect text boxes using EasyOCR and filter out large text boxes based on an area threshold.\n    Returns:\n    - text_mask: a binary mask with text regions marked (as a numpy array).\n    \"\"\"\n    image_np = np.array(image)\n    results = reader.readtext(image_np)\n    text_mask = np.zeros_like(image_np[:, :, 0])\n\n    for (bbox, text, prob) in results:\n        x0, y0 = bbox[0]\n        x1, y1 = bbox[1]\n        x2, y2 = bbox[2]\n        x3, y3 = bbox[3]\n        width = np.linalg.norm([x1 - x0, y1 - y0])\n        height = np.linalg.norm([x2 - x1, y2 - y1])\n        area = width * height\n\n        if area < area_threshold:\n            pts = np.array(bbox, dtype=np.int32)\n            cv2.fillPoly(text_mask, [pts], 255)\n\n    return text_mask\n\ndef add_text_channel(images):\n    processed_images = []\n    for img in images:\n        if img.shape[0] == 3:\n            img_pil = transforms.ToPILImage()(img.cpu())\n            text_mask = detect_text_boxes_with_easyocr(img_pil)\n            text_mask_resized = cv2.resize(text_mask, (img_pil.size[0], img_pil.size[1]))\n            text_mask_resized = text_mask_resized / 255.0\n            text_mask_tensor = transforms.ToTensor()(text_mask_resized).to(img.device)\n            img_with_text_channel = torch.cat([img, text_mask_tensor], dim=0)\n        else:\n            img_with_text_channel = img\n        \n        processed_images.append(img_with_text_channel)\n    \n    return torch.stack(processed_images)\n\n\ndef apply_gaussian_blur(saliency_map, kernel_size=(5, 5)):\n    return cv2.GaussianBlur(saliency_map, kernel_size, 0)\n\n\ndef process_validation_data(val_dataloader, output_folder, filenames):\n    create_directory(output_folder)\n\n    for i, (images, _) in enumerate(val_dataloader):\n        images = images.to(device)\n        images_with_text_channel = add_text_channel(images)\n        \n        with torch.no_grad():\n            saliency_maps = model1(images_with_text_channel)\n\n        for j in range(images.size(0)):\n            saliency_map = saliency_maps[j].squeeze().cpu().numpy()\n\n            # Normalize the saliency map between 0 and 1\n            saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())\n\n            # Apply Gaussian blur to the saliency map\n            saliency_map = apply_gaussian_blur(saliency_map)\n\n            # Convert to uint8 for saving\n            saliency_map = (saliency_map * 255).astype(\"uint8\")\n            \n            # Use the original filename\n            filename = filenames[i * images.size(0) + j]\n            output_path = os.path.join(output_folder, filename)\n\n            success = cv2.imwrite(output_path, saliency_map)\n            if success:\n                print(f'Successfully saved {filename} with shape {saliency_map.shape} to {output_path}')\n            else:\n                print(f'Failed to save image for {filename}')\n\nMODE = \"PRED\"\n\nif MODE == \"PRED\":\n    output_folder = \"/kaggle/working/preds8\"\nelse:\n    output_folder = \"/kaggle/working/preds/val\"\n\nfilenames = [f\"{name}.png\" for name in val_img_ids]\n\n# Process the validation DataLoader\nprocess_validation_data(val_dataloader, output_folder, filenames)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_folder = \"/kaggle/working/preds8\"  \ngt_folder = \"/kaggle/input/saliency-data/maps/val\" \n\n#threshold, normalize, gaussian\nmetrics = evaluate_model(predictions_folder, gt_folder, post_process_method=\"\")\nprint(f\"Model Evaluation Metrics with Post-Processing:\\nAUC: {metrics['AUC']}\\nNSS: {metrics['NSS']}\\nCC: {metrics['CC']}\\nKLD: {metrics['KLD']}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\n\ndef create_directory(directory):\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\ndef load_images_from_folder(folder):\n    images = {}\n    for filename in os.listdir(folder):\n        if filename.endswith(\".png\") or filename.endswith(\".jpg\"):  # Adjust for your image format\n            img_path = os.path.join(folder, filename)\n            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n            if img is not None:\n                images[filename] = img\n    return images\n\ndef ensemble_predictions(folder1, folder2, output_folder, method=\"average\"):\n\n    preds1 = load_images_from_folder(folder1)\n    preds2 = load_images_from_folder(folder2)\n\n    create_directory(output_folder)\n\n\n    common_filenames = list(set(preds1.keys()).intersection(set(preds2.keys())))\n\n    for filename in common_filenames:\n        pred1 = preds1[filename]\n        pred2 = preds2[filename]\n\n\n        if pred1.shape != pred2.shape:\n            print(f\"Resizing {filename} predictions to match dimensions.\")\n            pred2 = cv2.resize(pred2, (pred1.shape[1], pred1.shape[0]))\n\n\n        if method == \"average\":\n            ensemble_pred = (pred1.astype(np.float32) + pred2.astype(np.float32)) / 2\n        elif method == \"max\":\n            ensemble_pred = np.maximum(pred1, pred2)\n        else:\n            raise ValueError(f\"Unknown ensemble method: {method}\")\n\n\n        ensemble_pred = (ensemble_pred - ensemble_pred.min()) / (ensemble_pred.max() - ensemble_pred.min()) * 255\n        ensemble_pred = ensemble_pred.astype(np.uint8)\n\n        output_path = os.path.join(output_folder, filename)\n        success = cv2.imwrite(output_path, ensemble_pred)\n        if success:\n            print(f'Successfully saved ensembled prediction: {filename} to {output_path}')\n        else:\n            print(f'Failed to save ensembled prediction for {filename}')\n\n\nfolder_path_1 = \"/kaggle/working/preds9\"\nfolder_path_2 = \"/kaggle/working/preds8\"\noutput_folder = \"/kaggle/working/preds10\"\n\n#average,max\nensemble_predictions(folder_path_1, folder_path_2, output_folder, method=\"average\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T13:18:03.159000Z","iopub.execute_input":"2024-09-15T13:18:03.159380Z","iopub.status.idle":"2024-09-15T13:18:03.257523Z","shell.execute_reply.started":"2024-09-15T13:18:03.159350Z","shell.execute_reply":"2024-09-15T13:18:03.256628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\n\n\"\"\"\nimport zipfile\nimport os\n\ndef compress_images_to_zip(folder_path, output_zip):\n    # Create a ZipFile object\n    with zipfile.ZipFile(output_zip, 'w') as zipf:\n        # Iterate over all the files in the directory\n        for root, dirs, files in os.walk(folder_path):\n            for file in files:\n                # Create the full file path\n                file_path = os.path.join(root, file)\n                # Add file to the zip file\n                zipf.write(file_path, os.path.relpath(file_path, folder_path))\n    print(f'All images have been compressed into {output_zip}')\n\n\nfolder_path = \"/kaggle/working/preds10\"  # Folder containing the images to compress\noutput_zip = '/kaggle/working/saliency_maps20.zip'  # Name of the output zip file\n\n# Compress the images\ncompress_images_to_zip(folder_path, output_zip)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-15T13:18:13.099461Z","iopub.execute_input":"2024-09-15T13:18:13.100339Z","iopub.status.idle":"2024-09-15T13:18:13.112182Z","shell.execute_reply.started":"2024-09-15T13:18:13.100305Z","shell.execute_reply":"2024-09-15T13:18:13.111119Z"},"trusted":true},"execution_count":null,"outputs":[]}]}